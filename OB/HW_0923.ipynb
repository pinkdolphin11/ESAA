{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmJzOkGCxpiBbHbEsxJho9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinkdolphin11/ESAA/blob/main/HW_0923.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7. 앙상블 학습과 랜덤 포레스트"
      ],
      "metadata": {
        "id": "K9oy3P-DpjFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "일련의 예측기(분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있다. 일련의 예측기를 앙상블이라고 하고, 이를 앙상블 학습이라고 한다.\n",
        "\n",
        "랜덤 포레스트 : 결정 트리의 앙상블. 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련시킨다. 모든 개별 트리의 예측을 구한 다음 가장 많은 선택을 받은 클래스를 예측으로 삼는다."
      ],
      "metadata": {
        "id": "V5o9Dq0QqcJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 투표 기반 분류기"
      ],
      "metadata": {
        "id": "QcqgAOM7qTJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "직접 투표 분류기 : 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측. 다수결 투표로 정해지는 분류기.\n",
        "\n",
        "다수결 투표 분류기가 앙상블에 포함된 개별 분류기 중 가장 뛰어난 것보다 높은 정확도를 보일 때가 많다. 각 분류기가 약한 학습기일지라도, 충분히 많고 다양하다면 앙상블은 강한 학습기가 될 수 있다. 이는 큰 수의 법칙 원리로 이해할 수 있다. 하지만 이런 가정은 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없어야 가능하다. 하지만 같은 데이터로 훈련시킬 경우 이 가정이 맞지 않는다. 분류기들이 같은 종류의 오차를 만들기 쉽기 때문에 잘못된 클래스가 다수인 경우가 많아지고 앙상블의 정확도가 낮아진다. 앙상블 방법은 예측기가 가능한 한 독립적일 때 최고의 성능을 발휘하므로, 다양한 분류기를 얻기 위해 각기 다른 알고리즘으로 학습시켜야 한다."
      ],
      "metadata": {
        "id": "JbBClNcXqx7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# moons 데이터 불러오기\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "\n",
        "# train, test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "eenvEcI5s7xQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h6zPk9jonOu",
        "outputId": "05d155b7-4cc8-4bec-c0d8-29604947eadd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
              "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 여러 분류기를 조합하여 사이킷런의 투표 기반 분류기(VotingClassifier)를 만들고 훈련시킴\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='hard')\n",
        "voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 분류기의 테스트셋 정확도\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train,y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0qZ1YWJsgKb",
        "outputId": "b0b2a8c5-0126-423a-b9e4-5080ed7faf04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.864\n",
            "RandomForestClassifier 0.888\n",
            "SVC 0.896\n",
            "VotingClassifier 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "투표 기반 분류기가 다른 개별 분류기보다 성능이 높다.\n",
        "\n",
        "간접 투표 : 모든 분류기가 클래스의 확률을 예측할 수 있으면(predict_proba() 메서드가 있으면) 개별 분류기 예측들의 평균을 구해서 확률이 가장 높은 클래스를 예측할 수 있다. 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높다. voting='soft'로 지정해서 사용한다. SVC의 경우 probability=True로 추가적으로 지정해야 한다."
      ],
      "metadata": {
        "id": "SZeQMO2ivZCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 배깅과 페이스팅"
      ],
      "metadata": {
        "id": "dUsfKR9wxJXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다양한 분류기를 만드는 방법으로 각기 다른 훈련 알고리즘을 사용하는 것 말고도, 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것이 있다.\n",
        "\n",
        "배깅 : 훈련 세트에서 중복을 허용하여 샘플링하는 방식\n",
        "\n",
        "페이스팅 : 훈련 세트에서 중복을 허용하지 않고 샘플링하는 방식\n",
        "\n",
        "배깅, 페이스팅에서는 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있다.\n",
        "\n",
        "모든 예측기가 훈련을 마치면 앙상블은 모든 예측을 모아서 새로운 샘플에 대한 예측을 만드는데, 분류의 경우 통계적 최빈값, 회귀에 대해서는 평균을 계산한다. 이런 수집 함수를 통과하면 개별 예측기의 편향과 분산이 모두 감소한다. 일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 주러든다."
      ],
      "metadata": {
        "id": "2k6PP98DxSVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.1 사이킷런의 배깅과 페이스팅"
      ],
      "metadata": {
        "id": "u_S3zVT7yOZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결정 트리 분류기 500개의 앙상블을 훈련시킴\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,n_jobs=1)\n",
        "bag_clf.fit(X_train,y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "0DhXVfh3ySZS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.2 oob 평가"
      ],
      "metadata": {
        "id": "rBBWsBiTyxvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "oob(out of bag) 샘플 : 배깅에서 선택되지 않은 나머지 훈련 샘플. 예측기마다 모두 다르다.\n",
        "\n",
        "예측기가 훈련되는 동안은 oob 샘플을 사용하지 않으므로 별도의 검증 세트를 사용하지 않고 oob 샘플을 사용해 평가할 수 있다. 앙상블의 평가는 각 예측기의 oob 평가를 평균해서 얻는다."
      ],
      "metadata": {
        "id": "u8iVrAlay6SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,bootstrap=True,n_jobs=-1,oob_score=True) #oob_score=True : 훈련이 끝난 후 자동으로 oob 평가 수행\n",
        "bag_clf.fit(X_train,y_train)\n",
        "bag_clf.oob_score_ #평가 점수 결과는 oob_score_에 저장됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMXdOsMZzVgb",
        "outputId": "1119ac4b-8dbb-43b6-fbce-4bb20f9b8496"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9013333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy5tn-1WztLk",
        "outputId": "f4bb2526-58e6-4e7b-f675-8ad776a00e83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.896"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 비슷한 정확도를 얻었다."
      ],
      "metadata": {
        "id": "hFeZNRg2z3tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_decision_function_ #각 훈련 샘플의 클래스 확률 반환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShjWVSYBz6ea",
        "outputId": "85cc0e8d-a44e-4253-fff7-9be4416436d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36413043, 0.63586957],\n",
              "       [0.31791908, 0.68208092],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07894737, 0.92105263],\n",
              "       [0.38095238, 0.61904762],\n",
              "       [0.02162162, 0.97837838],\n",
              "       [0.98930481, 0.01069519],\n",
              "       [0.96907216, 0.03092784],\n",
              "       [0.76966292, 0.23033708],\n",
              "       [0.00512821, 0.99487179],\n",
              "       [0.75274725, 0.24725275],\n",
              "       [0.81920904, 0.18079096],\n",
              "       [0.96756757, 0.03243243],\n",
              "       [0.0273224 , 0.9726776 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.96629213, 0.03370787],\n",
              "       [0.95      , 0.05      ],\n",
              "       [0.98876404, 0.01123596],\n",
              "       [0.01010101, 0.98989899],\n",
              "       [0.40223464, 0.59776536],\n",
              "       [0.88571429, 0.11428571],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98314607, 0.01685393],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.60233918, 0.39766082],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00574713, 0.99425287],\n",
              "       [0.14754098, 0.85245902],\n",
              "       [1.        , 0.        ],\n",
              "       [0.0052356 , 0.9947644 ],\n",
              "       [0.36363636, 0.63636364],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.18324607, 0.81675393],\n",
              "       [0.35323383, 0.64676617],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01515152, 0.98484848],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98395722, 0.01604278],\n",
              "       [0.89351852, 0.10648148],\n",
              "       [0.96315789, 0.03684211],\n",
              "       [0.95811518, 0.04188482],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05641026, 0.94358974],\n",
              "       [0.95977011, 0.04022989],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00574713, 0.99425287],\n",
              "       [0.97209302, 0.02790698],\n",
              "       [0.81005587, 0.18994413],\n",
              "       [0.40828402, 0.59171598],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.72727273, 0.27272727],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.86127168, 0.13872832],\n",
              "       [1.        , 0.        ],\n",
              "       [0.61497326, 0.38502674],\n",
              "       [0.1005291 , 0.8994709 ],\n",
              "       [0.65384615, 0.34615385],\n",
              "       [0.89944134, 0.10055866],\n",
              "       [0.        , 1.        ],\n",
              "       [0.16463415, 0.83536585],\n",
              "       [0.89839572, 0.10160428],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05913978, 0.94086022],\n",
              "       [0.04188482, 0.95811518],\n",
              "       [0.30208333, 0.69791667],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01111111, 0.98888889],\n",
              "       [0.87203791, 0.12796209],\n",
              "       [0.005     , 0.995     ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.25531915, 0.74468085],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00497512, 0.99502488],\n",
              "       [0.        , 1.        ],\n",
              "       [0.91891892, 0.08108108],\n",
              "       [0.72674419, 0.27325581],\n",
              "       [0.00591716, 0.99408284],\n",
              "       [1.        , 0.        ],\n",
              "       [0.19895288, 0.80104712],\n",
              "       [0.62295082, 0.37704918],\n",
              "       [0.        , 1.        ],\n",
              "       [0.04812834, 0.95187166],\n",
              "       [0.53623188, 0.46376812],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01675978, 0.98324022],\n",
              "       [1.        , 0.        ],\n",
              "       [0.23469388, 0.76530612],\n",
              "       [0.47524752, 0.52475248],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01734104, 0.98265896],\n",
              "       [0.98469388, 0.01530612],\n",
              "       [0.28272251, 0.71727749],\n",
              "       [0.93604651, 0.06395349],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.80225989, 0.19774011],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01612903, 0.98387097],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98857143, 0.01142857],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99516908, 0.00483092],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.95081967, 0.04918033],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00546448, 0.99453552],\n",
              "       [0.28421053, 0.71578947],\n",
              "       [0.98285714, 0.01714286],\n",
              "       [0.25748503, 0.74251497],\n",
              "       [0.98484848, 0.01515152],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.74251497, 0.25748503],\n",
              "       [0.35051546, 0.64948454],\n",
              "       [0.4040404 , 0.5959596 ],\n",
              "       [0.82386364, 0.17613636],\n",
              "       [0.9494382 , 0.0505618 ],\n",
              "       [0.04907975, 0.95092025],\n",
              "       [0.77595628, 0.22404372],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02538071, 0.97461929],\n",
              "       [0.95876289, 0.04123711],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01212121, 0.98787879],\n",
              "       [0.        , 1.        ],\n",
              "       [0.01104972, 0.98895028],\n",
              "       [0.00492611, 0.99507389],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.92696629, 0.07303371],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99431818, 0.00568182],\n",
              "       [0.        , 1.        ],\n",
              "       [0.33809524, 0.66190476],\n",
              "       [0.24365482, 0.75634518],\n",
              "       [0.01111111, 0.98888889],\n",
              "       [0.        , 1.        ],\n",
              "       [0.36315789, 0.63684211],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99492386, 0.00507614],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00505051, 0.99494949],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98224852, 0.01775148],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.55248619, 0.44751381],\n",
              "       [0.93877551, 0.06122449],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98969072, 0.01030928],\n",
              "       [0.98907104, 0.01092896],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.07177033, 0.92822967],\n",
              "       [1.        , 0.        ],\n",
              "       [0.0060241 , 0.9939759 ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.045     , 0.955     ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.95698925, 0.04301075],\n",
              "       [0.75882353, 0.24117647],\n",
              "       [0.58139535, 0.41860465],\n",
              "       [0.        , 1.        ],\n",
              "       [0.15384615, 0.84615385],\n",
              "       [1.        , 0.        ],\n",
              "       [0.93820225, 0.06179775],\n",
              "       [0.97382199, 0.02617801],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00980392, 0.99019608],\n",
              "       [0.        , 1.        ],\n",
              "       [0.44692737, 0.55307263],\n",
              "       [0.88202247, 0.11797753],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99421965, 0.00578035],\n",
              "       [0.01630435, 0.98369565],\n",
              "       [0.        , 1.        ],\n",
              "       [0.95767196, 0.04232804],\n",
              "       [0.00564972, 0.99435028],\n",
              "       [0.29508197, 0.70491803],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00534759, 0.99465241],\n",
              "       [0.97297297, 0.02702703],\n",
              "       [0.87564767, 0.12435233],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07772021, 0.92227979],\n",
              "       [0.99421965, 0.00578035],\n",
              "       [0.02688172, 0.97311828],\n",
              "       [0.        , 1.        ],\n",
              "       [0.01081081, 0.98918919],\n",
              "       [1.        , 0.        ],\n",
              "       [0.80327869, 0.19672131],\n",
              "       [0.        , 1.        ],\n",
              "       [0.89637306, 0.10362694],\n",
              "       [0.98445596, 0.01554404],\n",
              "       [0.21019108, 0.78980892],\n",
              "       [0.22335025, 0.77664975],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.21546961, 0.78453039],\n",
              "       [0.98314607, 0.01685393],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98963731, 0.01036269],\n",
              "       [0.        , 1.        ],\n",
              "       [0.5       , 0.5       ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00492611, 0.99507389],\n",
              "       [0.        , 1.        ],\n",
              "       [0.09356725, 0.90643275],\n",
              "       [0.09090909, 0.90909091],\n",
              "       [0.98765432, 0.01234568],\n",
              "       [0.01036269, 0.98963731],\n",
              "       [1.        , 0.        ],\n",
              "       [0.40540541, 0.59459459],\n",
              "       [0.0959596 , 0.9040404 ],\n",
              "       [0.53475936, 0.46524064],\n",
              "       [0.55617978, 0.44382022],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.58823529, 0.41176471],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.20231214, 0.79768786],\n",
              "       [0.82608696, 0.17391304],\n",
              "       [0.03243243, 0.96756757],\n",
              "       [1.        , 0.        ],\n",
              "       [0.83516484, 0.16483516],\n",
              "       [0.        , 1.        ],\n",
              "       [0.01117318, 0.98882682],\n",
              "       [0.10752688, 0.89247312],\n",
              "       [0.03846154, 0.96153846],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98863636, 0.01136364],\n",
              "       [0.91326531, 0.08673469],\n",
              "       [0.19135802, 0.80864198],\n",
              "       [0.95555556, 0.04444444],\n",
              "       [0.01010101, 0.98989899],\n",
              "       [0.54255319, 0.45744681],\n",
              "       [0.05357143, 0.94642857],\n",
              "       [0.98404255, 0.01595745],\n",
              "       [0.85263158, 0.14736842],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.90340909, 0.09659091],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00537634, 0.99462366],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.27027027, 0.72972973],\n",
              "       [0.9939759 , 0.0060241 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.85635359, 0.14364641],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.76190476, 0.23809524],\n",
              "       [0.94059406, 0.05940594],\n",
              "       [1.        , 0.        ],\n",
              "       [0.67741935, 0.32258065],\n",
              "       [0.49122807, 0.50877193],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87373737, 0.12626263],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.85164835, 0.14835165],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.80829016, 0.19170984],\n",
              "       [0.08421053, 0.91578947],\n",
              "       [0.47311828, 0.52688172],\n",
              "       [0.16939891, 0.83060109],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87765957, 0.12234043],\n",
              "       [0.84146341, 0.15853659],\n",
              "       [0.00641026, 0.99358974],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99473684, 0.00526316],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02247191, 0.97752809],\n",
              "       [0.93989071, 0.06010929],\n",
              "       [0.96703297, 0.03296703],\n",
              "       [1.        , 0.        ],\n",
              "       [0.52331606, 0.47668394],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00540541, 0.99459459],\n",
              "       [0.98863636, 0.01136364],\n",
              "       [0.02840909, 0.97159091],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98492462, 0.01507538],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05405405, 0.94594595],\n",
              "       [0.00537634, 0.99462366],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99468085, 0.00531915],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.13414634, 0.86585366],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.38764045, 0.61235955],\n",
              "       [0.04444444, 0.95555556],\n",
              "       [0.24352332, 0.75647668],\n",
              "       [1.        , 0.        ],\n",
              "       [0.97560976, 0.02439024],\n",
              "       [0.24712644, 0.75287356],\n",
              "       [0.99514563, 0.00485437],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.92405063, 0.07594937],\n",
              "       [0.34065934, 0.65934066],\n",
              "       [0.99489796, 0.00510204],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99465241, 0.00534759],\n",
              "       [0.        , 1.        ],\n",
              "       [0.04519774, 0.95480226],\n",
              "       [0.97660819, 0.02339181],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00510204, 0.99489796],\n",
              "       [0.61081081, 0.38918919]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 랜덤 패치와 랜덤 서브스페이스"
      ],
      "metadata": {
        "id": "6Al3--Oh0BIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 패치 방식 : 훈련 특성과 샘플을 모두 샘플링하는 것\n",
        "\n",
        "랜덤 서브스페이스 방식 : 훈련 샘플을 모두 사용하고(bootstrap=False, max_samples=1.0) 특성은 샘플링(bootstrap_feaures=True 또는 max_features<1.0)하는 것\n",
        "\n",
        "특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춘다."
      ],
      "metadata": {
        "id": "Ap8dwdJG0HSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 랜덤 포레스트"
      ],
      "metadata": {
        "id": "h3EFrWif0cq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)\n",
        "rnd_clf.fit(X_train,y_train)\n",
        "y_pred_clf = rnd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "2oWmmAkb0gL5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features='auto',max_leaf_nodes=16),n_estimators=500,max_samples=1.0,bootstrap=True,n_jobs=-1)"
      ],
      "metadata": {
        "id": "welCENzd0w24"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.1 엑스트라 트리\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "BLbElO6L1CyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "익스트림 랜덤 트리 앙상블(엑스트라 트리) : 극단적으로 무작위한 트리의 랜덤 포레스트. 편향이 늘어나지만 분산은 낮아진다."
      ],
      "metadata": {
        "id": "cn0rC3021QBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.2 특성 중요도"
      ],
      "metadata": {
        "id": "ZNU3ndA61TL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
        "rnd_clf.fit(iris['data'],iris['target'])\n",
        "for name, score in zip(iris['feature_names'],rnd_clf.feature_importances_):\n",
        "  print(name,score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksCwOEi1V3Q",
        "outputId": "0e3b52e0-05ca-4bf0-9044-062d2a4f5326"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm) 0.08823272404432969\n",
            "sepal width (cm) 0.021101362932409475\n",
            "petal length (cm) 0.45626423147092715\n",
            "petal width (cm) 0.4344016815523336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 부스팅"
      ],
      "metadata": {
        "id": "yZFQgCj01rUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "부스팅 : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법. 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것이다."
      ],
      "metadata": {
        "id": "toNJv-kS1sw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5.1 에이다부스트"
      ],
      "metadata": {
        "id": "aee1cooH1y3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이면 새로운 예측기는 학습하기 어려운 샘플에 점점 맞춰지게 되는데, 이를 에이다부스트라고 한다. 예를 들어, 에이다부스트 분류기를 만들 때, 먼저 알고리즘이 기반이 되는 첫번째 분류기를 훈련 세트에서 훈련시키고 예측을 만든다. 그 다음 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높인다. 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만든다. 이런 식으로 가중치 업데이트를 반복한다.\n",
        "\n",
        "<예측기 가중치>\n",
        "\n",
        "$\\alpha_{j} = 학습률 하이퍼파라미터 * log\\frac{1-r_{j}}{r_{j}}$\n",
        "\n",
        "<에이다부스트 예측>\n",
        "\n",
        "$\\hat{y} = argmax\\Sigma_{j=1}^{N}\\alpha_{j}$\n",
        "\n",
        "N : 예측기 수"
      ],
      "metadata": {
        "id": "LJ08SFXK11AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,algorithm='SAMME.R',learning_rate=0.5)\n",
        "ada_clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUjmi4uP3k-3",
        "outputId": "68ec3ba6-4d18-428e-9f95-0757507914db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5.2 그레이디언트 부스팅"
      ],
      "metadata": {
        "id": "gt_PGA-m3zNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그레이디언트 부스팅 : 에이다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가하지만, 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킨다."
      ],
      "metadata": {
        "id": "qzg3Rhzf368R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X,y)\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X,y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gouK4m836p3",
        "outputId": "a559f66b-d3e6-4fb2-ce44-6b270509d359"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두번째 예측기가 만든 잔여 오차에 세 번째 회귀 모델 훈련\n",
        "\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X,y3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1S4MYB14ZOw",
        "outputId": "f65fd832-7523-4e5f-c303-869bb5ba9d22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 트리의 예측의 합\n",
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1,tree_reg2,tree_reg3))"
      ],
      "metadata": {
        "id": "5QHszI-e5Cex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "트리가 앙상블에 추가될수록 앙상블의 예측이 점점 좋아진다."
      ],
      "metadata": {
        "id": "xDLQxd5g5EJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)\n",
        "gbrt.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDWZP0li5H-G",
        "outputId": "0f53b9fa-c210-4973-90e0-ee3378b5a8e7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "축소 : learning_rate 매개변수를 낮게 설정하여 각 트리의 기여 정도를 조절하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아지는 방법"
      ],
      "metadata": {
        "id": "MwKVqNdP5UlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
        "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=120)\n",
        "gbrt.fit(X_train,y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators=np.argmin(errors)+1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljNUFb9l5nL_",
        "outputId": "4b13c3af-803b-4aae-f7b1-a776bbb00e99"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(max_depth=2, n_estimators=58)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 조기 종료 구현 (warm_start = True)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2,warm_start=True)\n",
        "\n",
        "min_val_error = float('inf')\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1,120):\n",
        "  gbrt.n_estimators = n_estimators\n",
        "  gbrt.fit(X_train,y_train)\n",
        "  y_pred = gbrt.predict(X_val)\n",
        "  val_error = mean_squared_error(y_val,y_pred)\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    error_going_up = 0\n",
        "  else:\n",
        "    error_going_up += 1\n",
        "    if error_going_up == 5:\n",
        "      break #조기 종료"
      ],
      "metadata": {
        "id": "oJkpSjCI6St6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률적 그레이디언트 부스팅 : 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정한다. 편향이 높아지는 대신 분산이 낮아지고 훈련 속도가 높아진다."
      ],
      "metadata": {
        "id": "KvTnYWlJ66AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train,y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfYn2X847DJT",
        "outputId": "90aaf4bd-21ab-4754-a188-92f4ec935ec3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03:52:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_reg.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbMJ7qTb7Pqm",
        "outputId": "a73b5ad6-3b15-4ed1-afb7-eb3fe006d37f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03:53:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:0.46686\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:0.43838\n",
            "[2]\tvalidation_0-rmse:0.41451\n",
            "[3]\tvalidation_0-rmse:0.394289\n",
            "[4]\tvalidation_0-rmse:0.376805\n",
            "[5]\tvalidation_0-rmse:0.361437\n",
            "[6]\tvalidation_0-rmse:0.348761\n",
            "[7]\tvalidation_0-rmse:0.338325\n",
            "[8]\tvalidation_0-rmse:0.329876\n",
            "[9]\tvalidation_0-rmse:0.322816\n",
            "[10]\tvalidation_0-rmse:0.316066\n",
            "[11]\tvalidation_0-rmse:0.310561\n",
            "[12]\tvalidation_0-rmse:0.305344\n",
            "[13]\tvalidation_0-rmse:0.301993\n",
            "[14]\tvalidation_0-rmse:0.298748\n",
            "[15]\tvalidation_0-rmse:0.29632\n",
            "[16]\tvalidation_0-rmse:0.292897\n",
            "[17]\tvalidation_0-rmse:0.29011\n",
            "[18]\tvalidation_0-rmse:0.287372\n",
            "[19]\tvalidation_0-rmse:0.284494\n",
            "[20]\tvalidation_0-rmse:0.282344\n",
            "[21]\tvalidation_0-rmse:0.280072\n",
            "[22]\tvalidation_0-rmse:0.277716\n",
            "[23]\tvalidation_0-rmse:0.276656\n",
            "[24]\tvalidation_0-rmse:0.274666\n",
            "[25]\tvalidation_0-rmse:0.272961\n",
            "[26]\tvalidation_0-rmse:0.271453\n",
            "[27]\tvalidation_0-rmse:0.270872\n",
            "[28]\tvalidation_0-rmse:0.270635\n",
            "[29]\tvalidation_0-rmse:0.27076\n",
            "[30]\tvalidation_0-rmse:0.270644\n",
            "Stopping. Best iteration:\n",
            "[28]\tvalidation_0-rmse:0.270635\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 스태킹"
      ],
      "metadata": {
        "id": "dZIBXvxr7beD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시키고 싶다는 아이디어에서 출발한다.\n",
        "\n",
        "새로운 샘플에 회귀 작업을 수행하는 앙상블에서, 기존의 세 예측기는 각각 다른 값을 예측하고 마지막 예측기(블렌더 또는 메타 학습기)가 이 예측을 입력으로 받아 최종 예측을 만든다. 블렌더는 일반적으로 홀드아웃 세트를 사용해서 학습시킨다. 먼저 훈련 세트를 2개의 서브셋으로 나누고, 첫번째 서브셋은 첫번째 레이어의 예측을 훈련시키기 위해 사용한다. 그 다음 첫번째 레이어의 예측기를 사용해 두번째(홀드아웃) 세트에 대한 예측을 만든다. 홀드아웃 세트의 각 샘플에 대해 3개의 예측값이 있는데, 타깃값은 그대로 쓰고 앞에서 예측한 값을 입력 특성으로 사용하는 새로운 훈련 세트를 만들 수 있다."
      ],
      "metadata": {
        "id": "UZvZiN0g7jI7"
      }
    }
  ]
}
